abs(PACF$acf) > (qt(p = .995, df = insampsize) / (insampsize)^.5)
##Cian - The AAPLlogRets dataframe was already created above, so there shouldn't be a
##        need to recreate it, and I've renamed it to AAPL
#########
#AAPLlogRets = data.frame(AAPL[-1,],LogRets = diff(log(AAPL$Adjusted)))
#pacf(AAPLlogRets$LogRets[insamp]) #Plotting PACF for Log Returns
#########
#This creates the PACF variable from the insample datasets log returns
PACF = pacf(AAPL$AAPLlogRets)
#This plots the PACF
PACF
abs(PACF$acf) > (qt(p = .995, df = insampsize) / (insampsize)^.5)
##Cian - Setting the seed makes the random variations reproducable
set.seed(123456)
##Cian - Get the sample size from the size of the dataset you're splitting
sample_size <- floor(nrow(AAPL) * 0.66)
##Cian - Get dataset references to the new indexes from that sample size (These indexes
##        reference 66% of the dataset)
sample_indexes <- sample(seq_len(nrow(AAPL)), size = sample_size)
##Cian - The newly created indexes are therefore the 'in sample' dataset and the 'out of
##        sample' dataset is just the remaining rows of the AAPL dataset
insamp <- AAPL[sample_indexes,]
outsamp <- AAPL[-sample_indexes,]
##Cian - The AAPLlogRets dataframe was already created above, so there shouldn't be a
##        need to recreate it, and I've renamed it to AAPL
#########
#AAPLlogRets = data.frame(AAPL[-1,],LogRets = diff(log(AAPL$Adjusted)))
#pacf(AAPLlogRets$LogRets[insamp]) #Plotting PACF for Log Returns
#########
#This creates the PACF variable from the insample datasets log returns
PACF = pacf(AAPL$AAPLlogRets)
#This plots the PACF
PACF
##Cian - The AAPLlogRets dataframe was already created above, so there shouldn't be a
##        need to recreate it, and I've renamed it to AAPL
#########
#AAPLlogRets = data.frame(AAPL[-1,],LogRets = diff(log(AAPL$Adjusted)))
#pacf(AAPLlogRets$LogRets[insamp]) #Plotting PACF for Log Returns
#########
#This creates the PACF variable from the insample datasets log returns
PACF = pacf(insample$AAPLlogRets)
#This plots the PACF
PACF
#PACF = pacf(AAPLlogRets$LogRets[insamp])
PACF #The PACF shows that the fourth variable is statistically significant
abs(PACF$acf) > (qt(p = .995, df = insampsize) / (insampsize)^.5)
##Cian - Setting the seed makes the random variations reproducable. You're randomly
##        splitting your data into in and out of sample variation but you don't want
##        you're results changing as you rerun your analyse. This is something to watch
##        out for when working with statistical modelling!
set.seed(123456)
##Cian - Get the sample size from the size of the dataset you're splitting
sample_size <- floor(nrow(AAPL) * 0.66)
##Cian - Get dataset references to the new indexes from that sample size (These indexes
##        reference 66% of the dataset)
sample_indexes <- sample(seq_len(nrow(AAPL)), size = sample_size)
##Cian - The newly created indexes are therefore the 'in sample' dataset and the 'out of
##        sample' dataset is just the remaining rows of the AAPL dataset
insamp <- AAPL[sample_indexes,]
outsamp <- AAPL[-sample_indexes,]
##Cian - The AAPLlogRets dataframe was already created above, so there shouldn't be a
##        need to recreate it, and I've renamed it to AAPL
#########
#AAPLlogRets = data.frame(AAPL[-1,],LogRets = diff(log(AAPL$Adjusted)))
#pacf(AAPLlogRets$LogRets[insamp]) #Plotting PACF for Log Returns
#########
#This creates the PACF variable from the insample datasets log returns
PACF = pacf(insample$AAPLlogRets)
##Cian - The AAPLlogRets dataframe was already created above, so there shouldn't be a
##        need to recreate it, and I've renamed it to AAPL
#########
#AAPLlogRets = data.frame(AAPL[-1,],LogRets = diff(log(AAPL$Adjusted)))
#pacf(AAPLlogRets$LogRets[insamp]) #Plotting PACF for Log Returns
#########
#This creates the PACF variable from the insample datasets log returns
PACF = pacf(insamp$AAPLlogRets)
#This plots the PACF
PACF
#PACF = pacf(AAPLlogRets$LogRets[insamp])
PACF #The PACF shows that the fourth variable is statistically significant
abs(PACF$acf) > (qt(p = .995, df = insampsize) / (insampsize)^.5)
##Cian - Setting the seed makes the random variations reproducable. You're randomly
##        splitting your data into in and out of sample variation but you don't want
##        you're results changing as you rerun your analyse. This is something to watch
##        out for when working with statistical modelling!
set.seed(1234567)
##Cian - Get dataset references to the new indexes from that sample size (These indexes
##        reference 66% of the dataset)
sample_indexes <- sample(seq_len(nrow(AAPL)), size = sample_size)
##Cian - The newly created indexes are therefore the 'in sample' dataset and the 'out of
##        sample' dataset is just the remaining rows of the AAPL dataset
insamp <- AAPL[sample_indexes,]
outsamp <- AAPL[-sample_indexes,]
##Cian - The AAPLlogRets dataframe was already created above, so there shouldn't be a
##        need to recreate it, and I've renamed it to AAPL
#########
#AAPLlogRets = data.frame(AAPL[-1,],LogRets = diff(log(AAPL$Adjusted)))
#pacf(AAPLlogRets$LogRets[insamp]) #Plotting PACF for Log Returns
#########
#This creates the PACF variable from the insample datasets log returns
PACF = pacf(insamp$AAPLlogRets)
#This plots the PACF
PACF
#PACF = pacf(AAPLlogRets$LogRets[insamp])
PACF #The PACF shows that the fourth variable is statistically significant
abs(PACF$acf) > (qt(p = .995, df = insampsize) / (insampsize)^.5)
#Task 7
ARmodel4 = arima(x = AAPLlogRets$LogRets[insamp], order = c(4,0,0),
transform.pars = F)
##Cian - The AAPLlogRets dataframe was already created above, so there shouldn't be a
##        need to recreate it, and I've renamed it to AAPL
#########
#AAPLlogRets = data.frame(AAPL[-1,],LogRets = diff(log(AAPL$Adjusted)))
#pacf(AAPLlogRets$LogRets[insamp]) #Plotting PACF for Log Returns
#########
#This creates the PACF variable from the insample datasets log returns
PACF = pacf(AAPL$AAPLlogRets)
#This plots the PACF
PACF
#PACF = pacf(AAPLlogRets$LogRets[insamp])
abs(PACF$acf) > (qt(p = .995, df = insampsize) / (insampsize)^.5)
##Cian - Get the sample size of the dataset (66% of number of rows in AAPL)
insampsize = floor(nrow(AAPL) * .66)
n = nrow(AAPL)
AAPL$timeIndex = 1:n
##Cian - Get the sample size of the dataset (66% of number of rows in AAPL)
insampsize = floor(nrow(AAPL) * .66)
##Cian - Then you get the number of rows in AAPL
n = nrow(AAPL)
##Cian - You add a numbered column into AAPL from 1 to n, you then will use this
##        later to split the dataset into it's two periods
AAPL$timeIndex = 1:n
##Cian - You then split the dataset into in and out of sample sizes based on where the
##        time indexes fall. Though you don't need to calculate the outsampsize as you
##        just reuse the insampsize value again.
insample = AAPL$timeIndex <= insampsize
#outsampsize = floor(nrow(AAPLlogRets) * 0.34)
outsample = Log$timeIndex > insampsize
#outsampsize = floor(nrow(AAPLlogRets) * 0.34)
outsample = AAPL$timeIndex > insampsize
nrow(insample)
##Cian - You then split the dataset into in and out of sample sizes based on where the
##        time indexes fall. Though you don't need to calculate the outsampsize as you
##        just reuse the insampsize value again.
insample = AAPL$timeIndex <= insampsize
summary(insample)
##Cian - You then split the dataset into in and out of sample sizes based on where the
##        time indexes fall. Though you don't need to calculate the outsampsize as you
##        just reuse the insampsize value again.
insamp = AAPL[which(AAPL$timeIndex <= insampsize),]
summary(insamp)
nrow(insamp)
insamp$Date[1:100]
AAPL[1:100,]
##Cian - You then split the dataset into in and out of sample sizes based on where the
##        time indexes falls in relation to the sample size. Though you don't need to
##        calculate the outsampsize as you just reuse the insampsize value again.
##        The which() function returns which row indexes of AAPL have the below condition
##        as true, and then creates the insamp dataset based off which rows are returned.
##        Though you should be aware that the below splits the dataset 66% of the way through
##        and therefore insamp is beginning of time series to 66% of the way through and outsamp
##        is 66% of the way through to the latest values. Just something to be aware of!
insamp = AAPL[which(AAPL$timeIndex <= insampsize),]
##Cian - The AAPLlogRets dataframe was already created above, so there shouldn't be a
##        need to recreate it, and I've renamed it to AAPL
#########
#AAPLlogRets = data.frame(AAPL[-1,],LogRets = diff(log(AAPL$Adjusted)))
#pacf(AAPLlogRets$LogRets[insamp]) #Plotting PACF for Log Returns
#########
#This creates the PACF variable from the insample datasets log returns
PACF = pacf(insamp$AAPLlogRets)
#This plots the PACF
PACF
#PACF = pacf(AAPLlogRets$LogRets[insamp])
abs(PACF$acf) > (qt(p = .995, df = insampsize) / (insampsize)^.5)
#outsampsize = floor(nrow(AAPLlogRets) * 0.34) - I'd recommend deleting this row
outsamp = AAPL[which(AAPL$timeIndex > insampsize),]
#PACF = pacf(AAPLlogRets$LogRets[insamp])
abs(PACF$acf) > (qt(p = .995, df = insampsize) / (insampsize)^.5)
#Task 7
##Cian - The below creates an ARIMA model which is an Auto Regressive Integrated Moving Average
##        model. This is referenced as ARIMA(p, d, q). Therefore by putting in c(4, 0, 0) it means
##        that you're ignoring the IMA part of ARIMA and just creating an AR(p) model with p=4 as
##        d = 0 and q = 0 in that case.
##        For more information I found a great website that describes ARIMA models in more detail:
##        https://people.duke.edu/~rnau/411arim.htm
##        The above notes overall look like a really good resource for understanding forecasting better!
ARmodel = arima(x = AAPL$LogRets[insamp], order = c(4, 0, 0), transform.pars = F)
summary(AAPL)
AAPL = get.hist.quote(instrument = "AAPL", start = "2002-01-01",
end = "2018-06-30", provider = "yahoo",
quote = "Adjusted", compression = "d") #Requesting historical data from Yahoo
AAPL = data.frame(Date = time(AAPL), AAPL)
#Task 2
sum(is.na(AAPL))  #No NAs counted
AAPL = na.omit(AAPL) #Omitting any NAs
#Task 3
##Cian - I renamed the AAPLlogRets just to AAPL dataset to make it more clear as that dataset
##        doesn't just contain the log odd values and seperated out the AAPLlogRets calculation.
##        I did that just to make your code easier for someone not familiar with it to read
logRets = diff(log(AAPL$Adjusted))
AAPL = data.frame(AAPL[-1,], logRets)
## Cian - The head() function looks at the top rows of the dataframe so you can have an example of
##          how your data is looking. This is usually done for validation purposes. Therefore, it's
##          not required here but still good to include for yourself :)
head(AAPL)
##Cian - Get the sample size of the dataset (66% of number of rows in AAPL)
insampsize = floor(nrow(AAPL) * .66)
##Cian - Then you get the number of rows in AAPL
n = nrow(AAPL)
##Cian - You add a numbered column into AAPL from 1 to n, you then will use this
##        later to split the dataset into it's two periods
AAPL$timeIndex = 1:n
##Cian - You then split the dataset into in and out of sample sizes based on where the
##        time indexes falls in relation to the sample size. Though you don't need to
##        calculate the outsampsize as you just reuse the insampsize value again.
##        The which() function returns which row indexes of AAPL have the below condition
##        as true, and then creates the insamp dataset based off which rows are returned.
##        Though you should be aware that the below splits the dataset 66% of the way through
##        and therefore insamp is beginning of time series to 66% of the way through and outsamp
##        is 66% of the way through to the latest values. Just something to be aware of!
insamp = AAPL[which(AAPL$timeIndex <= insampsize),]
#outsampsize = floor(nrow(AAPLlogRets) * 0.34) - I'd recommend deleting this row
outsamp = AAPL[which(AAPL$timeIndex > insampsize),]
#This creates the PACF variable from the insample datasets log returns
PACF = pacf(insamp$logRets)
#This plots the PACF
PACF
##Cian - The below shows that there is a statistically significant correlation with each
##        lagged time series at the 4th value. This means that you should be building an AR(p)
##        model with p = 4 for the best results!
abs(PACF$acf) > (qt(p = .995, df = insampsize) / (insampsize)^.5)
#Task 7
##Cian - The below creates an ARIMA model which is an Auto Regressive Integrated Moving Average
##        model. This is referenced as ARIMA(p, d, q). Therefore by putting in c(4, 0, 0) it means
##        that you're ignoring the IMA part of ARIMA and just creating an AR(p) model with p=4 as
##        d = 0 and q = 0 in that case.
##        For more information I found a great website that describes ARIMA models in more detail:
##        https://people.duke.edu/~rnau/411arim.htm
##        The above notes overall look like a really good resource for understanding forecasting better!
ARmodel = arima(x = AAPL$logRets[insamp], order = c(4, 0, 0), transform.pars = F)
#Task 7
##Cian - The below creates an ARIMA model which is an Auto Regressive Integrated Moving Average
##        model. This is referenced as ARIMA(p, d, q). Therefore by putting in c(4, 0, 0) it means
##        that you're ignoring the IMA part of ARIMA and just creating an AR(p) model with p=4 as
##        d = 0 and q = 0 in that case.
##        For more information I found a great website that describes ARIMA models in more detail:
##        https://people.duke.edu/~rnau/411arim.htm
##        The above notes overall look like a really good resource for understanding forecasting better!
ARmodel = arima(x = insamp$logRets, order = c(4, 0, 0), transform.pars = F)
ARmodel
tScore = ARmodel4$coef/diag(ARmodel4$var.coef)^.5
tScore = ARmodel$coef/diag(ARmodel$var.coef)^.5
tScore
#Task 9
CoefCritVal = qt(p = 0.995, nrow(AAPL) - 5)
CoefCritVal
tScore
round((1-pnorm(abs(tScore),0,1))*2,4) # calculates the p-value for the coefficients.
which(abs(tScore) > CoefCritVal)
which(abs(tScore) > CoefCritVal)
round((1-pnorm(abs(tScore),0,1))*2, 4)
?arima
?arima()
#---------Refined model i.e (removing not significant coefficients)
ARModelRef = arima(x = insamp$logRets, order=c(4,0,0), transform.pars = F, fixed=c(0,NA,0,NA,0))# fixed() removes the coefficients
#marked with 0
ARModelRef
ARmodel
#---------Refined model i.e (removing not significant coefficients)
ARModelRef = arima(x = insamp$logRets, order=c(4,0,0), transform.pars = F, fixed=c(0,0,0,NA,NA))# fixed() removes the coefficients
#marked with 0
ARModelRef
ReftScore = ARModelRef$coef[ARModelRef$coef!=0]/diag(ARModelRef$var.coef)^.5
ReftScore
CoefCritVal
CoefCritVal = qt(p = 0.995, nrow(AAPL) - 5)
CoefCritVal
CoefCritVal = qt(p = 0.995, nrow(insamp) - 5)
CoefCritVal
#Task 1
"getSymbols.yahoo.warning"=FALSE
AAPL = get.hist.quote(instrument = "AAPL", start = "2002-01-01",
end = "2018-06-30", provider = "yahoo",
quote = "Adjusted", compression = "d") #Requesting historical data from Yahoo
AAPL = data.frame(Date = time(AAPL), AAPL)
#Task 2
sum(is.na(AAPL))  #No NAs counted
AAPL = na.omit(AAPL) #Omitting any NAs
#Task 3
##Cian - I renamed the AAPLlogRets just to AAPL dataset to make it more clear as that dataset
##        doesn't just contain the log odd values and seperated out the AAPLlogRets calculation.
##        I did that just to make your code easier for someone not familiar with it to read
logRets = diff(log(AAPL$Adjusted))
AAPL = data.frame(AAPL[-1,], logRets)
## Cian - The head() function looks at the top rows of the dataframe so you can have an example of
##          how your data is looking. This is usually done for validation purposes. Therefore, it's
##          not required here but still good to include for yourself :)
head(AAPL)
#Task 4
##Cian - Get the sample size of the dataset (66% of number of rows in AAPL)
insampsize = floor(nrow(AAPL) * .66)
##Cian - Then you get the number of rows in AAPL
n = nrow(AAPL)
##Cian - You add a numbered column into AAPL from 1 to n, you then will use this
##        later to split the dataset into it's two periods
AAPL$timeIndex = 1:n
##Cian - You then split the dataset into in and out of sample sizes based on where the
##        time indexes falls in relation to the sample size. Though you don't need to
##        calculate the outsampsize as you just reuse the insampsize value again.
##        The which() function returns which row indexes of AAPL have the below condition
##        as true, and then creates the insamp dataset based off which rows are returned.
##        Though you should be aware that the below splits the dataset 66% of the way through
##        and therefore insamp is beginning of time series to 66% of the way through and outsamp
##        is 66% of the way through to the latest values. Just something to be aware of!
insamp = AAPL[which(AAPL$timeIndex <= insampsize),]
#outsampsize = floor(nrow(AAPLlogRets) * 0.34) - I'd recommend deleting this row
outsamp = AAPL[which(AAPL$timeIndex > insampsize),]
#Task 5
##Cian - the assignment pdf specified that you should do all below tasks using the in sample
##        dataset unless specifically told otherwise so I've updated the below to follow this
##        instruction
##Cian - The AAPLlogRets dataframe was already created above, so there shouldn't be a
##        need to recreate it, and I've renamed it to AAPL
######### -I'd recommend deleting this block of code, but I kept it here for reference
#AAPLlogRets = data.frame(AAPL[-1,],LogRets = diff(log(AAPL$Adjusted)))
#pacf(AAPLlogRets$LogRets[insamp]) #Plotting PACF for Log Returns
#########
#This creates the PACF variable from the insample datasets log returns
PACF = pacf(insamp$logRets)
#This plots the PACF
PACF
#Task 6
##Cian - You were meant to create the PACF in the above task so I moved it up a level.
##        Also, the PACF shows the relation of the dataset to lagged time series values.
##        I'd recommend reading up on the PACF and ACF functions in greater detail to
##        improve your understanding of what these functions represent. It's not that the
##        fourth coefficient of the dataset has significance, it's that within the dataset
##        there is a pattern that each value 4 values apart has correlation with each other
##        that's why it's called the partial autocorrelation function. I found a great website
##        below that helps describe this better:
##        https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/time-series/how-to/partial-autocorrelation/interpret-the-results/partial-autocorrelation-function-pacf/
##Cian - The below shows that there is a statistically significant correlation with each
##        lagged time series at the 4th value. This means that you should be building an AR(p)
##        model with p = 4 for the best results!
abs(PACF$acf) > (qt(p = .995, df = insampsize) / (insampsize)^.5)
#Task 7
##Cian - The below creates an ARIMA model which is an Auto Regressive Integrated Moving Average
##        model. This is referenced as ARIMA(p, d, q). Therefore by putting in c(4, 0, 0) it means
##        that you're ignoring the IMA part of ARIMA and just creating an AR(p) model with p=4 as
##        d = 0 and q = 0 in that case.
##        For more information I found a great website that describes ARIMA models in more detail:
##        https://people.duke.edu/~rnau/411arim.htm
##        The above notes overall look like a really good resource for understanding forecasting better!
ARModel = arima(x = insamp$logRets, order = c(4, 0, 0), transform.pars = F)
#Task 8
##Cian - The below tScore is calculated against all the coefficients of the ARmodel so as to understand
##        the significane they represent to the model
tScore = ARModel$coef/diag(ARModel$var.coef)^.5
tScore #Calculating the tScore you can see that the fourth coefficient is a lot larger than the rest
#Task 9
##Cian - You then calculate the threshold ('Critical value') which represent which coefficent tScore
##        values are signficant
CoefCritVal = qt(p = 0.995, nrow(AAPL) - 5)
CoefCritVal
#Task10
##Cian - The below calculates the p-score values for each of the coefficients as they would be compared
##        with the CoefCritVal scores. This is a fully valid way of testing which coefficients have
##        significance to the model, but the assignment question specifically asked you to use the
##        comparison of the CoefCritVal comparisons with tScore values to tell which ones are signficant
##        This means you need to test which absolute values for tScore are greater than the CoefCritVal.
##        i.e. |tScore| > CoefCritVal
##        Therefore, I would do a check using the which() function as shown below
#This shows that the fourth coefficient and the intercept have statistical significance
which(abs(tScore) > CoefCritVal)
##Cian - The below is fully valid so I'm going to leave it there for reference, but I'd recommend deleting
##        it in the final version of your assignment submission. This would show that your p-value for the
##        fourth coefficient and the intercept are < 0.01 (Your alpha/ significance value) and therefore
##        they are statistically significant as they are values that are considered outside the normal
##        range of values that would be expected from that distribution (The normal distribution in this case)
##        A good starting resource for understanding p-values is shown below:
##        http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-correctly-interpret-p-values
#########
# calculates the p-value for the coefficients.
#pnorm() calculates the cdf (Cian - Cumulative distribution function, I'd recommend reading up on this!)
#at a specific z-value. abs() returns the absolute value of the z-value. We multiple the reuslt by 2 because
#it is a two-tail test and round() rounds the values to the 4th decimal place
#round((1-pnorm(abs(tScore),0,1))*2, 4)
#########
#Task11
##Cian - This code is fairly sound, but they also got rid of the intercept, even though that was shown to be
##        statistically significant. I changed the code slightly to include the intercept, and to reference
##        the insamp dataset
#Refined model i.e (removing not significant coefficients)
# fixed() removes the coefficients marked with 0
ARModelRef = arima(x = insamp$logRets, order=c(4,0,0), transform.pars = F, fixed=c(0,0,0,NA,NA))
ARModelRef
#Task12
##Cian - The ony change was to the new model variable name
ReftScore = ARModelRef$coef[ARModelRef$coef!=0]/diag(ARModelRef$var.coef)^.5
ReftScore
#Task13
##Cian - The only change I made was the it references the number of rows in insamp intead of AAPL
CoefCritVal = qt(p = 0.995, nrow(insamp) - 5)
CoefCritVal
#Task14
Box.test(ARModel4ref$residuals,lag = log(insampsize), fitdf = 1)
#Task14
##Cian - I've never used the Ljung-Box test before, but the below seems to look good to me :-P
Box.test(ARModelRef$residuals,lag = log(insampsize), fitdf = 1)
var(insamp$logRets)
predict(ARModelRef,5)  # prediction 5 steps ahead
tsdisplay(residuals(ARModelRef), lag.max=45, main='(1,1,1) Model Residuals')
forecase(ARModelRef, h=5)
forecast(ARModelRef, h=5)
forecast(ARModelRef, h=5)
ARModelRef
predict(ARModelRef,5)
?predict
ARModelRef
?forecast
forecast(ARModelRef, h=5)
predict(ARModelRef, n.ahead = 5)
?arima
?predict
Pred = predict(outsample, model = ARModelRef)
Pred = arima(outsample, model = ARModelRef)
plot(predict(ARModelRef, n.ahead = 5))
FivePred <- predict(ARModelRef, n.ahead = 5)
Fivepred
FivePred
FivePred$pred
FivePred$pred[1]
plot(FivePred$pred)
Pred = Arima(outsample, model = ARModelRef)
summary(outsamp)
Pred = Arima(outsamp$logRets, model = ARModelRef)
Pred
predict(Pred, n.head = nrow(outsamp))
nrow(outsamp)
predict(Pred, n.ahead = nrow(outsamp))
temp.pred <- predict(Pred, n.ahead = nrow(outsamp))
plot(temp.pred$pred)
?Arima
Pred
accuracy(Pred)
accuracy(ARModelRef)
forecast(Pred, h = nrow(outsamp))
Pred.forecast <- forecast(Pred, h = nrow(outsamp))
plot(Pred.forecast)
?forecast
forecast(Pred)
Pred.forecast <- forecast(Pred)
plot(Pred.forecast)
?xts
?diff
?qt
setwd("~/GitHub/datasciencenanodegree/1 Supervised Learning/5 Support Vector Machines")
?svm
??svm
svm??
s
??support
library('caret')
install.packages('caret')
library(caret)
setwd("~/GitHub/datasciencenanodegree/1 Supervised Learning/5 Support Vector Machines")
data <- read.csv('data_test.csv', header=FALSE)
data[,3]
summary(data)
?trainControl
?train
data <- read.csv('data_test.csv', header=FALSE)
set.seed(42)
intrain <- createDataPartition(y = data[,3], p = 0.67, list = FALSE)
training <- data[intrain,]
testing <- data[-intrain,]
trCtrl <- trainControl(method = 'repeatedcv', number = 10, repeats = 3)
svm_linear <- train(data[,3] ~ ., data = data, method = 'svmlinear',
trControl = trCtrl,
preProcess = c('center','scale'),
tuneLength = 10)
svm_linear <- train(data[,3] ~ ., data = data, method = 'svmLinear',
trControl = trCtrl,
preProcess = c('center','scale'),
tuneLength = 10)
svm_linear <- train(data[,3] ~ data[,1] + data[,2], data = data, method = 'svmLinear',
trControl = trCtrl,
preProcess = c('center','scale'),
tuneLength = 10)
data_df <- read.csv('data_test.csv', header=FALSE)
set.seed(42)
intrain <- createDataPartition(y = data_df[,3], p = 0.67, list = FALSE)
training <- data_df[intrain,]
testing <- data_df[-intrain,]
trCtrl <- trainControl(method = 'repeatedcv', number = 10, repeats = 3)
svm_linear <- train(data[,3] ~ ., data = data_df, method = 'svmLinear',
trControl = trCtrl,
preProcess = c('center','scale'),
tuneLength = 10)
?train''
?train
data_df[,3] <- as.factor(data_df[,3])
set.seed(42)
intrain <- createDataPartition(y = data_df[,3], p = 0.67, list = FALSE)
training <- data_df[intrain,]
testing <- data_df[-intrain,]
trCtrl <- trainControl(method = 'repeatedcv', number = 10, repeats = 3)
svm_linear <- train(data[,3] ~ ., data = data_df, method = 'svmLinear',
trControl = trCtrl,
preProcess = c('center','scale'),
tuneLength = 10)
